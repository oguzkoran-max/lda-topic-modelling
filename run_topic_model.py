#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LDA Topic Modelling â€” Ä°talyanca Edebiyat Metinleri
===================================================
Lemma-tabanlÄ±, deterministik, tekrar edilebilir analiz.
DH Metodoloji Ã–ÄŸretim AracÄ± felsefesine uygun: Ã¶ÄŸrenci algoritmayÄ±
Ã§alÄ±ÅŸÄ±rken gÃ¶rÃ¼r, parametreleri deÄŸiÅŸtirir, sonuÃ§larÄ± gÃ¶zlemler.

KullanÄ±m:
    streamlit run run_topic_model.py
"""
from __future__ import annotations

import sys
import io
import subprocess
import zipfile
import platform
from datetime import datetime

from collections import Counter

import numpy as np
import pandas as pd
import spacy
import sklearn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import streamlit as st
import altair as alt
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Dosya formatlarÄ±
from odf.opendocument import load as odf_load
from odf.text import P as odf_P
from odf import teletype
import pdfplumber

# Coherence
from gensim.corpora import Dictionary as GensimDictionary
from gensim.models.coherencemodel import CoherenceModel

# â”€â”€ SABÄ°T PARAMETRELER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SEED = 42
DEFAULT_N_TOPICS = 5
DEFAULT_MAX_ITER = 20
DEFAULT_CHUNK_SIZE = 200
MIN_DF = 2
MAX_DF = 0.85
MIN_TOKEN_LEN = 3
TOP_N_WORDS = 10

LANG_MODELS = {
    "Ä°talyanca": "it_core_news_sm",
    "Ä°ngilizce": "en_core_web_sm",
}

ALL_POS_TAGS = ["NOUN", "VERB", "ADJ", "ADV", "PROPN"]
DEFAULT_POS = ["NOUN", "VERB", "ADJ"]

np.random.seed(SEED)


# â”€â”€ YARDIMCI FONKSÄ°YONLAR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_environment_report(spacy_model: str) -> str:
    """Ã‡alÄ±ÅŸma ortamÄ± bilgilerini raporla."""
    import gensim
    nlp_meta = spacy.load(spacy_model).meta
    lines = [
        f"Tarih/Saat          : {datetime.now().isoformat()}",
        f"Platform            : {platform.platform()}",
        f"Python              : {sys.version}",
        f"numpy               : {np.__version__}",
        f"scikit-learn        : {sklearn.__version__}",
        f"pandas              : {pd.__version__}",
        f"spaCy               : {spacy.__version__}",
        f"gensim              : {gensim.__version__}",
        f"spaCy model         : {nlp_meta['lang']}_{nlp_meta['name']} v{nlp_meta['version']}",
        f"random_state (seed) : {SEED}",
    ]
    return "\n".join(lines)


def get_model_parameters(
    spacy_model: str,
    effective_min_df: int,
    effective_max_df: float,
    actual_topics: int,
    max_iter: int,
    chunk_size: int,
    pos_tags: list[str],
    custom_sw: set[str],
) -> str:
    """Model ve Ã¶n-iÅŸleme parametrelerini raporla."""
    lines = [
        "â•â•â• Ã–N-Ä°ÅLEME â•â•â•",
        f"spaCy model           : {spacy_model}",
        f"KÃ¼Ã§Ã¼k harfe Ã§evirme   : Evet",
        f"Noktalama Ã§Ä±karma     : Evet",
        f"SayÄ± Ã§Ä±karma          : Evet",
        f"Stopword temizleme    : Evet (spaCy + Ã¶zel)",
        f"Ã–zel stopword         : {', '.join(sorted(custom_sw)) if custom_sw else 'Yok'}",
        f"POS filtresi          : {', '.join(pos_tags)}",
        f"Lemmatizasyon         : Evet",
        f"Min. token uzunluÄŸu   : {MIN_TOKEN_LEN}",
        "",
        "â•â•â• BELGE PARÃ‡ALAMA â•â•â•",
        f"ParÃ§a boyutu (kelime) : {chunk_size}",
        f"Min. parÃ§a uzunluÄŸu   : 20 kelime",
        "",
        "â•â•â• VEKTÃ–RÄ°ZASYON (CountVectorizer) â•â•â•",
        f"min_df                : {effective_min_df}",
        f"max_df                : {effective_max_df}",
        f"ngram_range           : (1, 1)",
        "",
        "â•â•â• MODEL (LatentDirichletAllocation) â•â•â•",
        f"n_components          : {actual_topics}",
        f"random_state          : {SEED}",
        f"learning_method       : batch",
        f"max_iter              : {max_iter}",
    ]
    return "\n".join(lines)


# â”€â”€ Dosya okuma â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def read_odt(file_bytes: bytes) -> str:
    doc = odf_load(io.BytesIO(file_bytes))
    paragraphs = doc.getElementsByType(odf_P)
    return "\n".join(teletype.extractText(p) for p in paragraphs)


def read_pdf(file_bytes: bytes) -> str:
    try:
        text_parts = []
        with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
            for page in pdf.pages:
                t = page.extract_text()
                if t:
                    text_parts.append(t)
        return "\n".join(text_parts)
    except Exception as exc:
        raise ValueError(f"PDF okunamadÄ± (dosya bozuk veya ÅŸifreli olabilir): {exc}")


def read_uploaded_file(uploaded_file) -> tuple[str, str | None]:
    """DosyayÄ± oku. (metin, hata_mesajÄ±) dÃ¶ndÃ¼rÃ¼r."""
    name = uploaded_file.name.lower()
    raw = uploaded_file.read()
    try:
        if name.endswith(".odt"):
            return read_odt(raw), None
        if name.endswith(".pdf"):
            return read_pdf(raw), None
        for enc in ("utf-8", "latin-1", "iso-8859-9"):
            try:
                return raw.decode(enc), None
            except UnicodeDecodeError:
                continue
        return raw.decode("utf-8", errors="replace"), None
    except Exception as exc:
        return "", str(exc)


# â”€â”€ NLP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@st.cache_resource
def load_spacy_model(model_name: str):
    """spaCy modelini yÃ¼kle; bulunmazsa otomatik indir."""
    try:
        nlp = spacy.load(model_name)
    except OSError:
        subprocess.check_call(
            [sys.executable, "-m", "spacy", "download", model_name]
        )
        nlp = spacy.load(model_name)
    nlp.max_length = 2_000_000
    return nlp


def preprocess(
    text: str,
    nlp,
    allowed_pos: set[str],
    custom_stopwords: set[str],
) -> list[str]:
    """Tek bir belgeyi Ã¶n-iÅŸle, temiz token listesi dÃ¶ndÃ¼r."""
    doc = nlp(text.lower())
    tokens = []
    for token in doc:
        if token.is_punct or token.is_space or token.is_stop:
            continue
        if token.like_num or token.text.isdigit():
            continue
        if token.pos_ not in allowed_pos:
            continue
        lemma = token.lemma_.strip()
        if lemma in custom_stopwords:
            continue
        if len(lemma) >= MIN_TOKEN_LEN and lemma.isalpha():
            tokens.append(lemma)
    return tokens


def preprocess_with_trace(
    text: str,
    nlp,
    allowed_pos: set[str],
    custom_stopwords: set[str],
) -> pd.DataFrame:
    """Ã–n-iÅŸleme adÄ±mlarÄ±nÄ± satÄ±r satÄ±r gÃ¶steren tablo Ã¼ret."""
    doc = nlp(text.lower())
    rows = []
    for token in doc:
        if token.is_space:
            continue
        lemma = token.lemma_.strip()
        is_punct = token.is_punct
        is_stop = token.is_stop or lemma in custom_stopwords
        is_num = token.like_num or token.text.isdigit()
        is_short = len(lemma) < MIN_TOKEN_LEN
        pos_ok = token.pos_ in allowed_pos
        kept = (
            not is_punct and not is_stop and not is_num
            and not is_short and pos_ok and lemma.isalpha()
        )
        rows.append({
            "Token": token.text,
            "Lemma": lemma,
            "POS": token.pos_,
            "Stopword": is_stop,
            "Noktalama": is_punct,
            "SayÄ±": is_num,
            "KÄ±sa (<3)": is_short,
            "POS Filtre": not pos_ok,
            "Dahil": kept,
        })
    return pd.DataFrame(rows)


def split_into_chunks(tokens: list[str], chunk_size: int = 200) -> list[list[str]]:
    chunks = []
    for i in range(0, len(tokens), chunk_size):
        chunk = tokens[i : i + chunk_size]
        if len(chunk) >= 20:
            chunks.append(chunk)
    return chunks


# â”€â”€ Analiz yardÄ±mcÄ±larÄ± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_topics_text(model, feature_names, n_words=TOP_N_WORDS):
    lines = []
    for idx, topic_dist in enumerate(model.components_):
        top_indices = topic_dist.argsort()[: -n_words - 1 : -1]
        top_words = [feature_names[i] for i in top_indices]
        lines.append(f"Topic {idx}: {', '.join(top_words)}")
    return "\n".join(lines)


def compute_coherence(lda_model, feature_names, token_lists, n_words=TOP_N_WORDS):
    """C_v coherence skoru hesapla (gensim)."""
    try:
        topics_words = []
        for topic_dist in lda_model.components_:
            top_idx = topic_dist.argsort()[: -n_words - 1 : -1]
            topics_words.append([feature_names[i] for i in top_idx])
        dictionary = GensimDictionary(token_lists)
        cm = CoherenceModel(
            topics=topics_words,
            texts=token_lists,
            dictionary=dictionary,
            coherence="c_v",
        )
        return cm.get_coherence()
    except Exception:
        return None


def compute_corpus_stats(
    raw_texts: list[str],
    all_token_lists: list[list[str]],
    nlp,
) -> dict:
    """Derlem istatistiklerini hesapla."""
    # Ham metin istatistikleri
    raw_combined = " ".join(raw_texts)
    raw_words = raw_combined.split()
    raw_word_count = len(raw_words)
    raw_char_count = len(raw_combined)

    # TemizlenmiÅŸ token istatistikleri
    all_tokens_flat = [t for tl in all_token_lists for t in tl]
    clean_token_count = len(all_tokens_flat)
    clean_type_count = len(set(all_tokens_flat))

    # Type-Token Ratio
    ttr = clean_type_count / clean_token_count if clean_token_count > 0 else 0.0

    # Frekans daÄŸÄ±lÄ±mÄ±
    freq = Counter(all_tokens_flat)
    top_50 = freq.most_common(50)

    # Hapax legomena (yalnÄ±zca 1 kez geÃ§en)
    hapax = sum(1 for w, c in freq.items() if c == 1)

    # Ortalama parÃ§a uzunluÄŸu
    avg_chunk_len = np.mean([len(tl) for tl in all_token_lists])

    return {
        "raw_word_count": raw_word_count,
        "raw_char_count": raw_char_count,
        "clean_token_count": clean_token_count,
        "clean_type_count": clean_type_count,
        "ttr": ttr,
        "top_50": top_50,
        "hapax_count": hapax,
        "avg_chunk_len": avg_chunk_len,
        "freq": freq,
    }


def generate_wordcloud(words_weights: dict[str, float]) -> plt.Figure:
    """Kelime bulutu Ã¼ret ve matplotlib Figure dÃ¶ndÃ¼r."""
    wc = WordCloud(
        width=600,
        height=300,
        background_color="white",
        colormap="viridis",
        max_words=50,
        random_state=SEED,
    ).generate_from_frequencies(words_weights)
    fig, ax = plt.subplots(figsize=(6, 3))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    plt.tight_layout(pad=0)
    return fig


def create_zip(files: dict[str, str]) -> bytes:
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
        for fname, content in files.items():
            zf.writestr(f"outputs/{fname}", content)
    return buf.getvalue()


def render_pyldavis(lda_model, dtm, vectorizer) -> str | None:
    try:
        import pyLDAvis
        import pyLDAvis.lda_model as lda_vis
        vis_data = lda_vis.prepare(lda_model, dtm, vectorizer, sort_topics=False)
        return pyLDAvis.prepared_data_to_html(vis_data)
    except ImportError:
        return None
    except Exception:
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STREAMLIT ARAYÃœZÃœ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.set_page_config(
    page_title="LDA Topic Modelling â€” DH Ã–ÄŸretim AracÄ±",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded",
)

# â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.sidebar.title("ğŸ“š LDA Topic Modelling")
st.sidebar.caption("Ankara Ãœniversitesi â€” Dijital BeÅŸeri Bilimler")
st.sidebar.markdown(
    '<span style="font-size:0.82em; color:#64748b;">'
    'Ã–ÄŸr. GÃ¶r. Dr. OÄŸuz KORAN<br>'
    'Dr. Ã–ÄŸr. Ãœyesi BarÄ±ÅŸ YÃœCESAN</span>',
    unsafe_allow_html=True,
)
st.sidebar.divider()

# Dil
lang_label = st.sidebar.selectbox(
    "ğŸŒ Metin Dili",
    list(LANG_MODELS.keys()),
    help="Metnin yazÄ±ldÄ±ÄŸÄ± dili seÃ§in. spaCy dil modeli buna gÃ¶re belirlenir: "
         "tokenizasyon, lemmatizasyon ve stopword listesi seÃ§ilen dile gÃ¶re deÄŸiÅŸir.\n\n"
         "**Neden dil seÃ§imi kritik?**\n\n"
         "Lemmatizasyon dile Ã¶zgÃ¼dÃ¼r: Ä°talyanca 'andavo' â†’ 'andare' dÃ¶nÃ¼ÅŸÃ¼mÃ¼, "
         "ancak Ä°talyanca modelle doÄŸru yapÄ±lÄ±r. YanlÄ±ÅŸ dil modeli = yanlÄ±ÅŸ lemmalar = "
         "anlamsÄ±z topic'ler.\n\n"
         "**Model boyutu:** Bu araÃ§ `sm` (small) modelleri kullanÄ±r. Daha bÃ¼yÃ¼k modeller "
         "(lg) daha doÄŸru lemmatizasyon yapar ancak yÃ¼kleme sÃ¼resi ve bellek kullanÄ±mÄ± artar.",
)
spacy_model = LANG_MODELS[lang_label]

# POS filtresi
st.sidebar.markdown("### ğŸ·ï¸ POS Filtresi")
pos_tags = st.sidebar.multiselect(
    "Dahil edilecek sÃ¶zcÃ¼k tÃ¼rleri",
    ALL_POS_TAGS,
    default=DEFAULT_POS,
    help="YalnÄ±zca seÃ§ili sÃ¶zcÃ¼k tÃ¼rlerindeki kelimeler analize dahil edilir.\n\n"
         "â€¢ **NOUN** (Ä°sim): Topic modelling'de en bilgilendirici tÃ¼r. "
         "Akademik Ã§alÄ±ÅŸmalarÄ±n Ã§oÄŸu yalnÄ±zca isimlerle baÅŸlar.\n\n"
         "â€¢ **VERB** (Fiil): Eylem temalarÄ± yakalar (Ã¶r. 'combattere', 'amare'). "
         "AnlatÄ± analizi iÃ§in Ã¶nemlidir.\n\n"
         "â€¢ **ADJ** (SÄ±fat): Duygusal ve tanÄ±mlayÄ±cÄ± temalarÄ± gÃ¼Ã§lendirir. "
         "Sentiment-yakÄ±n analiz iÃ§in faydalÄ±.\n\n"
         "â€¢ **ADV** (Zarf): Genellikle gÃ¼rÃ¼ltÃ¼ ekler, dikkatli kullanÄ±n. "
         "Akademik Ã§alÄ±ÅŸmalarda nadiren dahil edilir.\n\n"
         "â€¢ **PROPN** (Ã–zel Ä°sim): Karakter/yer adlarÄ±nÄ± dahil eder. "
         "NER (Named Entity Recognition) tarzÄ± analizlerde yararlÄ±.\n\n"
         "**Akademik tercih:** Schofield ve Mimno (2016) Ã§alÄ±ÅŸmasÄ±, "
         "POS filtresinin topic kalitesini Ã¶nemli Ã¶lÃ§Ã¼de artÄ±rdÄ±ÄŸÄ±nÄ± gÃ¶stermiÅŸtir. "
         "**Sadece NOUN** ile baÅŸlamak akademik standarttÄ±r. "
         "ArdÄ±ndan NOUN+ADJ, sonra NOUN+ADJ+VERB deneyin. "
         "Her seferinde coherence skorunun nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶zlemleyin.\n\n"
         "> Schofield, A. & Mimno, D. (2016). 'Comparing Apples to Apple.' "
         "ACL Workshop on NLP+CSS.",
)
if not pos_tags:
    pos_tags = DEFAULT_POS
    st.sidebar.warning("En az bir POS tÃ¼rÃ¼ gerekli. VarsayÄ±lan kullanÄ±lÄ±yor.")
allowed_pos_set = set(pos_tags)

# Ã–zel stopword
st.sidebar.markdown("### ğŸš« Ã–zel Stopword")
custom_sw_input = st.sidebar.text_input(
    "Ek stopword (virgÃ¼lle ayÄ±rÄ±n)",
    placeholder="Ã¶r: dire, fare, cosa, essere",
    help="spaCy'nin varsayÄ±lan stopword listesine ek olarak Ã§Ä±karmak istediÄŸiniz kelimeler.\n\n"
         "**Neden gerekli?** spaCy'nin genel stopword listesi gÃ¼nlÃ¼k dil iÃ§in tasarlanmÄ±ÅŸtÄ±r. "
         "Edebiyat metinlerinde Ã§ok sÄ±k geÃ§en ama tematik anlam taÅŸÄ±mayan kelimeler "
         "(Ã¶r. Ä°talyancada 'dire', 'fare', 'cosa', 'essere', 'molto', 'ancora') "
         "ek olarak Ã§Ä±karÄ±lmalÄ±dÄ±r.\n\n"
         "**YÃ¶ntem (iteratif stopword refinement):**\n"
         "1. Analizi ilk kez stopword eklemeden Ã§alÄ±ÅŸtÄ±rÄ±n.\n"
         "2. Topic kelimelerine bakÄ±n â†’ birden fazla topic'te tekrar eden anlamsÄ±z kelimeler var mÄ±?\n"
         "3. Bu kelimeleri buraya ekleyin, tekrar Ã§alÄ±ÅŸtÄ±rÄ±n.\n"
         "4. Coherence skoru yÃ¼kseldiyse doÄŸru yoldasÄ±nÄ±z.\n\n"
         "**Dikkat:** Tematik anlam taÅŸÄ±yan kelimeleri Ã§Ä±karmayÄ±n. "
         "Ã–rneÄŸin 'morte' (Ã¶lÃ¼m) sÄ±k geÃ§ebilir ama tematik bir sinyal taÅŸÄ±r.",
)
custom_stopwords = {w.strip().lower() for w in custom_sw_input.split(",") if w.strip()}

# Parametreler
st.sidebar.markdown("### âš™ï¸ Parametreler")

n_topics = st.sidebar.slider(
    "Konu sayÄ±sÄ± (K)", 2, 15, DEFAULT_N_TOPICS, key="k",
    help="LDA'nÄ±n kaÃ§ farklÄ± konu (topic) arayacaÄŸÄ±nÄ± belirler. "
         "Bu, modelin en kritik hiperparametresidir.\n\n"
         "â€¢ **K Ã§ok dÃ¼ÅŸÃ¼kse** â†’ farklÄ± konular tek topic altÄ±nda birleÅŸir "
         "(under-fitting). Topic'ler Ã§ok genel ve belirsiz olur.\n\n"
         "â€¢ **K Ã§ok yÃ¼ksekse** â†’ konular anlamsÄ±z parÃ§alara bÃ¶lÃ¼nÃ¼r "
         "(over-fitting). Benzer topic'ler Ã§oÄŸalÄ±r.\n\n"
         "**Optimal K nasÄ±l bulunur?**\n\n"
         "1. **Coherence (C_v) yÃ¶ntemi:** FarklÄ± K deÄŸerleri (3, 5, 7, 10) deneyin. "
         "En yÃ¼ksek coherence skorunu veren K'yÄ± seÃ§in. Bu yÃ¶ntem RÃ¶der et al. (2015) "
         "tarafÄ±ndan Ã¶nerilmiÅŸtir.\n\n"
         "2. **Ä°nsani deÄŸerlendirme:** Topic kelimelerine bakÄ±n â€” her topic'e anlamlÄ± "
         "bir etiket verebiliyor musunuz? VeremiyorsanÄ±z K'yÄ± deÄŸiÅŸtirin.\n\n"
         "3. **pyLDAvis kontrolÃ¼:** Daireler net biÃ§imde ayrÄ±ÅŸÄ±yorsa K uygun. "
         "Ãœst Ã¼ste biniyorsa K'yÄ± dÃ¼ÅŸÃ¼rÃ¼n.\n\n"
         "**Akademik standart:** Ã‡oÄŸu DH Ã§alÄ±ÅŸmasÄ±, farklÄ± K deÄŸerleriyle yapÄ±lan "
         "deneyleri ve seÃ§im gerekÃ§elerini raporlar. Tek bir K ile sonuÃ§ Ã§Ä±karmak "
         "metodolojik olarak zayÄ±f kabul edilir.",
)

n_words = st.sidebar.slider(
    "Konu baÅŸÄ±na kelime", 5, 20, TOP_N_WORDS, key="w",
    help="Her topic iÃ§in gÃ¶sterilecek en aÄŸÄ±rlÄ±klÄ± kelime sayÄ±sÄ±.\n\n"
         "Bu parametre modeli **deÄŸiÅŸtirmez**, yalnÄ±zca gÃ¶sterim derinliÄŸini ayarlar.\n\n"
         "â€¢ **10 kelime** (varsayÄ±lan): Topic'in ana temasÄ±nÄ± gÃ¶rmek iÃ§in yeterli.\n\n"
         "â€¢ **15â€“20 kelime**: Daha ince ayrÄ±mlarÄ± gÃ¶rmek iÃ§in. Alt temalar gÃ¶rÃ¼nÃ¼r hale gelir.\n\n"
         "**Akademik not:** Ã‡oÄŸu akademik yayÄ±nda topic'ler 10â€“15 kelimeyle raporlanÄ±r.",
)

chunk_size = st.sidebar.slider(
    "ParÃ§a boyutu (kelime)", 50, 500, DEFAULT_CHUNK_SIZE, key="chunk",
    help="Metin kaÃ§ kelimelik parÃ§alara bÃ¶lÃ¼nsÃ¼n? "
         "Bu parametre Ã¶zellikle tek belge yÃ¼klendiÄŸinde kritiktir.\n\n"
         "â€¢ **KÃ¼Ã§Ã¼k parÃ§a (50â€“100):** Mikro tematik deÄŸiÅŸimleri yakalar ama gÃ¼rÃ¼ltÃ¼lÃ¼ olabilir. "
         "KÄ±sa ÅŸiirler veya paragraf dÃ¼zeyinde analiz iÃ§in uygundur.\n\n"
         "â€¢ **Orta parÃ§a (150â€“250):** Roman ve uzun denemeler iÃ§in Ã¶nerilen aralÄ±k. "
         "Tematik ayrÄ±mlarÄ± yakalarken yeterli istatistiksel gÃ¼Ã§ saÄŸlar.\n\n"
         "â€¢ **BÃ¼yÃ¼k parÃ§a (300â€“500):** Genel temalarÄ± yakalar ama ince ayrÄ±mlar kaybolur. "
         "Ã‡ok uzun metinlerde veya birden fazla eserde kullanÄ±lÄ±r.\n\n"
         "**Akademik baÄŸlam:** ParÃ§a boyutu, LDA'nÄ±n 'belge' olarak neyi gÃ¶rdÃ¼ÄŸÃ¼nÃ¼ belirler. "
         "Jockers (2013) *Macroanalysis* kitabÄ±nda parÃ§a boyutunun sonuÃ§larÄ± Ã¶nemli Ã¶lÃ§Ã¼de "
         "etkilediÄŸini gÃ¶stermiÅŸtir. FarklÄ± boyutlarla deney yapmanÄ±z Ã¶nerilir.\n\n"
         "**Minimum parÃ§a:** 20 kelimeden kÄ±sa parÃ§alar otomatik olarak atlanÄ±r "
         "(istatistiksel olarak anlamsÄ±zdÄ±r).",
)

max_iter = st.sidebar.slider(
    "LDA iterasyon", 5, 50, DEFAULT_MAX_ITER, key="iter",
    help="Modelin eÄŸitim dÃ¶ngÃ¼sÃ¼ sayÄ±sÄ± (kaÃ§ kez veri Ã¼zerinden geÃ§er).\n\n"
         "â€¢ **10â€“20 iterasyon:** KÃ¼Ã§Ã¼k ve orta korpuslar iÃ§in genellikle yeterli.\n\n"
         "â€¢ **30â€“50 iterasyon:** Ã‡ok bÃ¼yÃ¼k veya karmaÅŸÄ±k korpuslarda gerekebilir.\n\n"
         "**YakÄ±nsama (convergence):** Model her iterasyonda biraz daha iyi hale gelir. "
         "EÄŸer sonuÃ§lar iterasyon artÄ±rÄ±ldÄ±ÄŸÄ±nda deÄŸiÅŸmiyorsa, model yakÄ±nsamÄ±ÅŸ demektir. "
         "Gereksiz yere yÃ¼ksek iterasyon Ã§alÄ±ÅŸma sÃ¼resini uzatÄ±r.\n\n"
         "**Not:** Bu araÃ§ `batch` Ã¶ÄŸrenme kullanÄ±r â€” tÃ¼m veri her iterasyonda bir kez "
         "iÅŸlenir. Bu yÃ¶ntem deterministiktir (online'a kÄ±yasla) ve akademik tekrar "
         "edilebilirlik iÃ§in tercih edilir.",
)

st.sidebar.markdown("---")
st.sidebar.markdown(f"**Seed:** `{SEED}` Â· **Model:** `{spacy_model}`")
st.sidebar.caption("AynÄ± veri + aynÄ± parametreler = aynÄ± sonuÃ§.")


# â”€â”€ Ana Alan â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.title("ğŸ“š LDA Topic Modelling")
st.markdown("**Ä°talyanca edebiyat metinleri iÃ§in lemma-tabanlÄ±, deterministik analiz**")

with st.expander("ğŸ“– LDA Nedir?", expanded=False):
    st.markdown("""
**Latent Dirichlet Allocation (LDA)**, metinlerdeki gizli *konularÄ±* (topics)
bulmaya Ã§alÄ±ÅŸan bir olasÄ±lÄ±ksal modeldir. Ä°lk kez **Blei, Ng ve Jordan (2003)**
tarafÄ±ndan Ã¶nerilmiÅŸtir ve Dijital BeÅŸeri Bilimler'de (*Digital Humanities*) en
yaygÄ±n kullanÄ±lan *topic modelling* tekniÄŸidir.

**Temel varsayÄ±mlar:**
- Her **belge** = konularÄ±n bir karÄ±ÅŸÄ±mÄ± (Ã¶r. %40 aÅŸk, %30 savaÅŸ, %30 doÄŸa)
- Her **konu** = kelimelerin bir olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± (Ã¶r. "aÅŸk" konusu: *cuore, amore, dolore, ...*)
- Model, kelime birliktelik Ã¶rÃ¼ntÃ¼lerinden bu gizli yapÄ±yÄ± **istatistiksel olarak** Ã§Ä±karÄ±r

**Ne yapar?**
LDA, "bu kelimeler birlikte sÄ±k geÃ§iyorsa muhtemelen aynÄ± konuya aittir"
mantÄ±ÄŸÄ±yla Ã§alÄ±ÅŸÄ±r. *cuore*, *amore* ve *passione* kelimeleri birlikte gÃ¶rÃ¼lÃ¼yorsa,
bunlarÄ± bir "konu kÃ¼mesi" olarak gruplar.

**Generative (Ã¼retici) model:**
LDA ÅŸu soruyu sorar: "Bu metinleri Ã¼reten gizli konu yapÄ±sÄ± ne olabilir?"
Bu nedenle **Ã¼retici** (generative) bir modeldir â€” metni olasÄ±lÄ±ksal olarak
yeniden Ã¼retmeye Ã§alÄ±ÅŸÄ±r ve gerÃ§ek metne en yakÄ±n konu daÄŸÄ±lÄ±mÄ±nÄ± bulur.

**Bag-of-Words (BoW) varsayÄ±mÄ±:**
LDA, kelime **sÄ±rasÄ±nÄ±** dikkate almaz. YalnÄ±zca kelimelerin belgede **kaÃ§ kez**
geÃ§tiÄŸine bakar. Bu sÄ±nÄ±rlama bilinÃ§li bir tercihtir: kelimelerin konumsal
iliÅŸkisini gÃ¶z ardÄ± etmek, bÃ¼yÃ¼k Ã¶lÃ§ekli tematik Ã¶rÃ¼ntÃ¼leri yakalamayÄ±
kolaylaÅŸtÄ±rÄ±r.

**Tek belge yÃ¼klediÄŸinizde:** Metin otomatik olarak parÃ§alara bÃ¶lÃ¼nÃ¼r
(her parÃ§a bir "pseudo-belge" olur) ve LDA bu parÃ§alar Ã¼zerinde Ã§alÄ±ÅŸÄ±r.
Bu yÃ¶ntem *distant reading* (uzak okuma) yaklaÅŸÄ±mÄ±yla uyumludur:
metnin mikro-bÃ¶lÃ¼mlerindeki tematik deÄŸiÅŸimleri gÃ¶zlemlemenizi saÄŸlar.

> **Referans:** Blei, D.M., Ng, A.Y. & Jordan, M.I. (2003). "Latent Dirichlet Allocation."
> *Journal of Machine Learning Research*, 3, 993â€“1022.
""")

with st.expander("ğŸ”¬ Ã–n-Ä°ÅŸleme AdÄ±mlarÄ±", expanded=False):
    st.markdown("""
```
Ham Metin
   â†“  KÃ¼Ã§Ã¼k harfe Ã§evirme
   â†“  Tokenizasyon (spaCy)
   â†“  Noktalama ve sayÄ±larÄ± Ã§Ä±karma
   â†“  Stopword temizleme (spaCy + Ã¶zel liste)
   â†“  POS filtresi (yalnÄ±zca seÃ§ili tÃ¼rler)
   â†“  Lemmatizasyon
   â†“  Min. 3 karakter filtresi
Temiz Token Listesi
```

**Neden her adÄ±m gerekli?**

| AdÄ±m | Neden? | Ã–rnek |
|------|--------|-------|
| **KÃ¼Ã§Ã¼k harf** | "Roma" ve "roma" aynÄ± kelime olarak sayÄ±lsÄ±n | Roma â†’ roma |
| **Tokenizasyon** | Metni anlamlÄ± birimlere ayÄ±rma | "l'amore" â†’ "l'" + "amore" |
| **Noktalama Ã§Ä±karma** | VirgÃ¼l, nokta vb. konu bilgisi taÅŸÄ±maz | "casa," â†’ "casa" |
| **SayÄ± Ã§Ä±karma** | Rakamlar tematik deÄŸildir | "1943", "25" â†’ Ã§Ä±karÄ±lÄ±r |
| **Stopword** | Ã‡ok sÄ±k ama anlamsÄ±z kelimeler | "il", "di", "che", "non" |
| **POS filtresi** | YalnÄ±zca iÃ§erik kelimelerini tut | Ä°sim, Fiil, SÄ±fat |
| **Lemmatizasyon** | FarklÄ± Ã§ekimleri birleÅŸtir | "andavo", "andare", "andata" â†’ "andare" |
| **Min. 3 karakter** | Ã‡ok kÄ±sa tokenlarÄ± ele | "di", "a", "in" â†’ Ã§Ä±karÄ±lÄ±r |

**Akademik baÄŸlam:** Ã–n-iÅŸleme, topic modelling'in en kritik aÅŸamasÄ±dÄ±r.
Schofield, Magnusson ve Mimno (2017) gÃ¶stermiÅŸtir ki kÃ¶tÃ¼ Ã¶n-iÅŸleme,
model parametrelerinden daha fazla sonucu etkiler. Bu nedenle her adÄ±mÄ±
kontrol altÄ±nda tutmak Ã¶nemlidir â€” "AdÄ±m AdÄ±m" tablosu tam da bunu yapmanÄ±zÄ± saÄŸlar.
""")

with st.expander("ğŸ¯ Akademik Metodoloji: LDA NasÄ±l KullanÄ±lÄ±r?", expanded=False):
    st.markdown("""
**LDA, Dijital BeÅŸeri Bilimler'de nasÄ±l kullanÄ±lÄ±r?**

Topic modelling, edebiyat araÅŸtÄ±rmalarÄ±nda ÅŸu amaÃ§larla kullanÄ±lÄ±r:

1. **Tematik keÅŸif (*distant reading*):** BÃ¼yÃ¼k bir korpusta hangi konularÄ±n
   baskÄ±n olduÄŸunu otomatik tespit etme. Franco Moretti'nin *distant reading*
   kavramÄ±yla uyumludur: metni yakÄ±ndan okumadan Ã¶nce genel yapÄ±yÄ± kavramak.

2. **KarÅŸÄ±laÅŸtÄ±rmalÄ± analiz:** FarklÄ± yazarlarÄ±n, dÃ¶nemlerin veya eserlerin
   tematik profillerini karÅŸÄ±laÅŸtÄ±rma. Ã–rn. Pavese ile Sciascia'nÄ±n hangi
   konularda ayrÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rmek.

3. **Tematik dÃ¶nÃ¼ÅŸÃ¼m:** Bir eserin bÃ¶lÃ¼mleri arasÄ±nda konularÄ±n nasÄ±l
   deÄŸiÅŸtiÄŸini takip etme. Roman boyunca hangi temalarÄ±n yÃ¼kselip dÃ¼ÅŸtÃ¼ÄŸÃ¼nÃ¼ gÃ¶rmek.

**Akademik iÅŸ akÄ±ÅŸÄ± (Ã¶nerilen):**

```
1. Korpus hazÄ±rlÄ±ÄŸÄ± â†’ metin temizleme, format birliÄŸi
2. Ä°lk analiz       â†’ varsayÄ±lan parametrelerle Ã§alÄ±ÅŸtÄ±r
3. Ä°teratif iyileÅŸtirme:
   a. Topic kelimelerine bak â†’ anlamlÄ± mÄ±?
   b. AnlamsÄ±z kelimeler â†’ Ã¶zel stopword'e ekle
   c. Coherence skoru dÃ¼ÅŸÃ¼kse â†’ K'yÄ± veya POS'u deÄŸiÅŸtir
   d. pyLDAvis'te daireler Ã¼st Ã¼steyse â†’ K'yÄ± dÃ¼ÅŸÃ¼r
4. SonuÃ§larÄ± yorumla â†’ topic'lere etiket ver
5. Raporla â†’ parametreler + metrikler + gÃ¶rselleÅŸtirmeler
```

**Ã–nemli not:** LDA bir keÅŸif aracÄ±dÄ±r (*exploratory tool*). SonuÃ§lar kesin "doÄŸrular"
deÄŸil, yorumlanmasÄ± gereken *olasÄ±lÄ±ksal Ã¶rÃ¼ntÃ¼lerdir*. Topic'lerin ne anlama geldiÄŸine
araÅŸtÄ±rmacÄ± karar verir â€” model yalnÄ±zca kelime kÃ¼melerini sunar.
""")

with st.expander("âš ï¸ SÄ±k YapÄ±lan Hatalar", expanded=False):
    st.markdown("""
| Hata | Neden sorunlu? | Ã‡Ã¶zÃ¼m |
|------|----------------|-------|
| K'yÄ± sabitleyip hiÃ§ deÄŸiÅŸtirmemek | Optimal K veriye baÄŸlÄ±dÄ±r | En az 3 farklÄ± K deneyin, coherence'Ä± karÅŸÄ±laÅŸtÄ±rÄ±n |
| Stopword eklememek | "dire", "fare" gibi kelimeler topic'leri kirletir | Ä°lk Ã§alÄ±ÅŸtÄ±rmada topic kelimelerini inceleyin, tekrar eden anlamsÄ±zlarÄ± ekleyin |
| YalnÄ±zca perplexity'e bakmak | DÃ¼ÅŸÃ¼k perplexity â‰  anlamlÄ± topic'ler | Coherence (C_v) + insani yorumlama birlikte kullanÄ±n |
| Ã‡ok kÃ¼Ã§Ã¼k parÃ§a boyutu | Topic'ler gÃ¼rÃ¼ltÃ¼lÃ¼ ve tutarsÄ±z olur | Tek kitap iÃ§in en az 150 kelime |
| Ã‡ok bÃ¼yÃ¼k parÃ§a boyutu | Ä°nce tematik ayrÄ±mlar kaybolur | 500'den bÃ¼yÃ¼k nadiren gerekli |
| POS filtresiz Ã§alÄ±ÅŸmak | Edatlar, baÄŸlaÃ§lar topic'leri domine eder | En az NOUN filtresi kullanÄ±n |
| Tek bir Ã§alÄ±ÅŸtÄ±rma ile sonuÃ§ Ã§Ä±karmak | LDA parametrelere duyarlÄ±dÄ±r | Ä°teratif deneme yapÄ±n, farklÄ± ayarlarÄ± karÅŸÄ±laÅŸtÄ±rÄ±n |
| SonuÃ§larÄ± "kanÄ±t" olarak sunmak | LDA keÅŸif aracÄ±dÄ±r, ispat aracÄ± deÄŸil | "LDA ÅŸunu gÃ¶steriyor" yerine "LDA ÅŸuna iÅŸaret ediyor" kullanÄ±n |
""")

st.divider()

# Dosya yÃ¼kleme
uploaded_files = st.file_uploader(
    "Belge yÃ¼kleyin (.txt, .odt veya .pdf)",
    type=["txt", "odt", "pdf"],
    accept_multiple_files=True,
    help="Her dosya ayrÄ± bir belge olarak deÄŸerlendirilir. "
         "Tek dosya yÃ¼klerseniz metin otomatik parÃ§alanÄ±r.",
)

if uploaded_files:
    st.info(f"ğŸ“„ {len(uploaded_files)} dosya yÃ¼klendi.")

    if st.button("â–¶ Analizi BaÅŸlat", type="primary", use_container_width=True):

        # â”€â”€ 1. DosyalarÄ± oku â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with st.spinner("Dosyalar okunuyorâ€¦"):
            filenames = []
            raw_texts = []
            for f in uploaded_files:
                f.seek(0)
                text, err = read_uploaded_file(f)
                if err:
                    st.warning(f"âš ï¸ {f.name}: {err} â€” atlandÄ±.")
                    continue
                if not text.strip():
                    st.warning(f"âš ï¸ {f.name}: Dosya boÅŸ â€” atlandÄ±.")
                    continue
                filenames.append(f.name)
                raw_texts.append(text)

        if not raw_texts:
            st.error("HiÃ§bir dosyadan metin okunamadÄ±.")
            st.stop()
        st.success(f"âœ… {len(raw_texts)} belge okundu.")

        # â”€â”€ 2. spaCy Ã¶n-iÅŸleme â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with st.spinner(f"spaCy Ã¶n-iÅŸleme ({spacy_model})â€¦"):
            nlp = load_spacy_model(spacy_model)

            all_token_lists = []
            chunk_labels = []
            trace_dfs = {}

            for fname, raw in zip(filenames, raw_texts):
                tokens = preprocess(raw, nlp, allowed_pos_set, custom_stopwords)

                if len(trace_dfs) < 3:
                    preview_text = " ".join(raw.split()[:500])
                    trace_dfs[fname] = preprocess_with_trace(
                        preview_text, nlp, allowed_pos_set, custom_stopwords
                    )

                if len(tokens) > chunk_size:
                    chunks = split_into_chunks(tokens, chunk_size)
                    for ci, chunk in enumerate(chunks):
                        all_token_lists.append(chunk)
                        chunk_labels.append(f"{fname}_p{ci+1}")
                elif len(tokens) >= 20:
                    all_token_lists.append(tokens)
                    chunk_labels.append(fname)
                else:
                    st.warning(
                        f"âš ï¸ {fname}: Ã¶n-iÅŸleme sonrasÄ± Ã§ok az token "
                        f"({len(tokens)}), atlandÄ±."
                    )

        if not all_token_lists:
            st.error("HiÃ§bir belgede yeterli token bulunamadÄ±.")
            st.stop()

        n_chunks = len(all_token_lists)
        st.success(
            f"âœ… {n_chunks} metin parÃ§asÄ± hazÄ±r "
            f"({len(filenames)} belge â†’ {n_chunks} parÃ§a)"
        )

        # Pedagojik: Ã¶n-iÅŸleme tablolarÄ±
        if trace_dfs:
            st.subheader(
                "ğŸ” Ã–n-Ä°ÅŸleme: AdÄ±m AdÄ±m",
                help="Bu tablo metnin LDA'ya verilmeden Ã¶nce nasÄ±l dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼ÄŸÃ¼nÃ¼ gÃ¶sterir.\n\n"
                     "â€¢ **Token:** Orijinal kelime.\n\n"
                     "â€¢ **Lemma:** SÃ¶zlÃ¼k formu (Ã¶r. 'andavo' â†’ 'andare').\n\n"
                     "â€¢ **POS:** SÃ¶zcÃ¼k tÃ¼rÃ¼ (NOUN, VERB, ADJ vb.).\n\n"
                     "â€¢ **POS Filtre:** SeÃ§ili olmayan sÃ¶zcÃ¼k tÃ¼rleri.\n\n"
                     "â€¢ ğŸŸ¢ YeÅŸil = analize dahil | ğŸ”´ KÄ±rmÄ±zÄ± = Ã§Ä±karÄ±lan.",
            )
            for fname, tdf in trace_dfs.items():
                with st.expander(f"{fname} â€” ilk 500 kelime"):
                    def _hl(row):
                        color = "#dcfce7" if row.get("Dahil") else "#fee2e2"
                        return [f"background-color: {color}"] * len(row)
                    st.dataframe(
                        tdf.style.apply(_hl, axis=1),
                        use_container_width=True,
                        height=350,
                    )
                    n_kept = int(tdf["Dahil"].sum())
                    st.caption(
                        f"ğŸŸ¢ Dahil: {n_kept} | ğŸ”´ Ã‡Ä±karÄ±lan: {len(tdf) - n_kept}"
                    )

        # â”€â”€ 2b. Derlem Ä°statistikleri â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with st.spinner("Derlem istatistikleri hesaplanÄ±yorâ€¦"):
            corpus_stats = compute_corpus_stats(raw_texts, all_token_lists, nlp)

        st.subheader(
            "ğŸ“Š Derlem Ä°statistikleri",
            help="Analiz Ã¶ncesi derlem hakkÄ±nda temel nicel bilgiler.\n\n"
                 "Bu veriler, LDA sonuÃ§larÄ±nÄ± yorumlamak iÃ§in baÄŸlam saÄŸlar. "
                 "Akademik makalenizin veri tanÄ±tÄ±m bÃ¶lÃ¼mÃ¼nde bu istatistikleri raporlayÄ±n.\n\n"
                 "**Type-Token Ratio (TTR):** SÃ¶zcÃ¼ksel Ã§eÅŸitliliÄŸi Ã¶lÃ§er. "
                 "YÃ¼ksek TTR â†’ metin zengin ve Ã§eÅŸitli bir sÃ¶z varlÄ±ÄŸÄ± kullanÄ±yor. "
                 "DÃ¼ÅŸÃ¼k TTR â†’ metin daha tekrarlayÄ±cÄ±.\n\n"
                 "**Hapax Legomena:** YalnÄ±zca 1 kez geÃ§en kelimeler. "
                 "Dilbilimde Zipf yasasÄ±na gÃ¶re, herhangi bir korpusta kelimelerin bÃ¼yÃ¼k "
                 "bir oranÄ± yalnÄ±zca bir kez geÃ§er.",
        )

        # Ã–zet metrikler
        s1, s2, s3, s4 = st.columns(4)
        s1.metric(
            "Ham SÃ¶zcÃ¼k",
            f"{corpus_stats['raw_word_count']:,}".replace(",", "."),
            help="Ã–n-iÅŸleme Ã¶ncesi, ham metindeki toplam sÃ¶zcÃ¼k sayÄ±sÄ± (whitespace split).",
        )
        s2.metric(
            "Temiz Token",
            f"{corpus_stats['clean_token_count']:,}".replace(",", "."),
            help="Lemmatizasyon, POS filtresi ve stopword temizliÄŸi sonrasÄ± kalan token sayÄ±sÄ±. "
                 "Bu sayÄ±, LDA'ya girdi olarak verilen toplam kelime miktarÄ±dÄ±r.",
        )
        s3.metric(
            "Benzersiz Kelime (Type)",
            f"{corpus_stats['clean_type_count']:,}".replace(",", "."),
            help="TemizlenmiÅŸ tokenlar arasÄ±ndaki farklÄ± kelime sayÄ±sÄ±. "
                 "LDA'nÄ±n sÃ¶zlÃ¼k boyutunu etkiler.",
        )
        s4.metric(
            "TTR",
            f"{corpus_stats['ttr']:.3f}",
            help="Type-Token Ratio = benzersiz kelime / toplam token. "
                 "SÃ¶zcÃ¼ksel Ã§eÅŸitliliÄŸin temel gÃ¶stergesi.\n\n"
                 "â€¢ **> 0.20** â†’ Ã‡eÅŸitli sÃ¶z varlÄ±ÄŸÄ±\n\n"
                 "â€¢ **0.05 â€“ 0.20** â†’ Orta dÃ¼zey\n\n"
                 "â€¢ **< 0.05** â†’ TekrarlayÄ±cÄ± metin\n\n"
                 "âš ï¸ TTR, metin uzunluÄŸuna duyarlÄ±dÄ±r: uzun metinlerde doÄŸal olarak dÃ¼ÅŸer.",
        )

        s5, s6, s7, s8 = st.columns(4)
        s5.metric(
            "Karakter SayÄ±sÄ±",
            f"{corpus_stats['raw_char_count']:,}".replace(",", "."),
            help="Ham metindeki toplam karakter sayÄ±sÄ± (boÅŸluklar dahil).",
        )
        s6.metric(
            "Hapax Legomena",
            f"{corpus_stats['hapax_count']:,}".replace(",", "."),
            help="YalnÄ±zca 1 kez geÃ§en kelime sayÄ±sÄ±. "
                 "Zipf yasasÄ±na gÃ¶re, doÄŸal dil metinlerinde kelimelerin yaklaÅŸÄ±k "
                 "%40â€“60'Ä± hapax legomena'dÄ±r.",
        )
        s7.metric(
            "Hapax OranÄ±",
            f"{corpus_stats['hapax_count'] / corpus_stats['clean_type_count'] * 100:.1f}%"
            if corpus_stats['clean_type_count'] > 0 else "â€”",
            help="Hapax legomena / benzersiz kelime sayÄ±sÄ±. "
                 "DoÄŸal dil metinlerinde %40â€“60 arasÄ± normaldir.",
        )
        s8.metric(
            "Ort. ParÃ§a UzunluÄŸu",
            f"{corpus_stats['avg_chunk_len']:.0f} token",
            help="LDA'ya verilen parÃ§alarÄ±n ortalama token sayÄ±sÄ±.",
        )

        # En sÄ±k 50 kelime â€” bar chart
        with st.expander("ğŸ“ˆ En SÄ±k 50 Kelime (Frekans DaÄŸÄ±lÄ±mÄ±)", expanded=True):
            st.markdown(
                "Ã–n-iÅŸleme sonrasÄ± **en sÄ±k geÃ§en 50 kelime**. "
                "Bu liste, LDA'ya girdi olan temizlenmiÅŸ sÃ¶zlÃ¼kteki daÄŸÄ±lÄ±mÄ± gÃ¶sterir."
            )
            top_50_df = pd.DataFrame(
                corpus_stats["top_50"], columns=["Kelime", "Frekans"]
            )
            if not top_50_df.empty:
                freq_chart = (
                    alt.Chart(top_50_df)
                    .mark_bar(color="#2563EB")
                    .encode(
                        x=alt.X("Frekans:Q", title="Frekans"),
                        y=alt.Y("Kelime:N", sort="-x", title="Kelime"),
                        tooltip=["Kelime", "Frekans"],
                    )
                    .properties(height=max(400, len(top_50_df) * 18))
                )
                st.altair_chart(freq_chart, use_container_width=True)

                st.dataframe(
                    top_50_df.style.format({"Frekans": "{:,}"}),
                    use_container_width=True,
                    height=300,
                )

            st.info(
                "ğŸ’¡ **Ä°pucu:** Bu listede birden fazla topic'te tekrar eden anlamsÄ±z kelimeler "
                "gÃ¶rÃ¼yorsanÄ±z, onlarÄ± sol paneldeki **Ã¶zel stopword** alanÄ±na ekleyin ve "
                "analizi yeniden Ã§alÄ±ÅŸtÄ±rÄ±n."
            )

        with st.expander("ğŸ“– Derlem Ä°statistikleri Ne Anlama Geliyor?"):
            st.markdown(f"""
**Derlem profili Ã¶zeti:**

| Ä°statistik | DeÄŸer | AÃ§Ä±klama |
|---|---|---|
| Ham sÃ¶zcÃ¼k sayÄ±sÄ± | {corpus_stats['raw_word_count']:,} | Ã–n-iÅŸleme Ã¶ncesi toplam sÃ¶zcÃ¼k |
| Temiz token sayÄ±sÄ± | {corpus_stats['clean_token_count']:,} | LDA'ya girdi olan kelime sayÄ±sÄ± |
| Benzersiz kelime (type) | {corpus_stats['clean_type_count']:,} | FarklÄ± kelime sayÄ±sÄ± |
| Type-Token Ratio | {corpus_stats['ttr']:.4f} | SÃ¶zcÃ¼ksel Ã§eÅŸitlilik |
| Hapax legomena | {corpus_stats['hapax_count']:,} | YalnÄ±zca 1 kez geÃ§en kelimeler |
| Filtreleme oranÄ± | {(1 - corpus_stats['clean_token_count'] / corpus_stats['raw_word_count']) * 100:.1f}% | Ham metinden ne kadarÄ± elendi |

**Bu sayÄ±larÄ± nasÄ±l yorumlamalÄ±?**

1. **Filtreleme oranÄ±:** Ã–n-iÅŸleme, ham metnin yaklaÅŸÄ±k %{(1 - corpus_stats['clean_token_count'] / corpus_stats['raw_word_count']) * 100:.0f}'ini eledi.
   Bu normaldir â€” Ä°talyanca metinlerde stopword, noktalama ve fonksiyon kelimeleri
   metnin %60â€“80'ini oluÅŸturur.

2. **TTR ({corpus_stats['ttr']:.3f}):** {"YÃ¼ksek sÃ¶zcÃ¼ksel Ã§eÅŸitlilik â€” metin zengin bir sÃ¶z varlÄ±ÄŸÄ± kullanÄ±yor." if corpus_stats['ttr'] > 0.20 else "Orta/dÃ¼ÅŸÃ¼k TTR â€” uzun metinlerde bu normaldir (tekrar eden kelimeler artar)." }

3. **Hapax oranÄ±:** Benzersiz kelimelerin %{corpus_stats['hapax_count'] / corpus_stats['clean_type_count'] * 100:.0f}'i yalnÄ±zca bir kez geÃ§iyor.
   Bu, doÄŸal dil metinlerinde beklenen bir Zipf daÄŸÄ±lÄ±mÄ±dÄ±r.

**Akademik baÄŸlam:** Derlem istatistikleri, sonuÃ§larÄ±nÄ±zÄ± kontekstÃ¼alize eder.
Ã‡ok kÃ¼Ã§Ã¼k bir derlem (< 1.000 token) Ã¼zerinde LDA sonuÃ§larÄ± gÃ¼venilir olmayabilir.
Genel kural: en az **2.000â€“5.000 temiz token** ile Ã§alÄ±ÅŸmak Ã¶nerilir.

> **Referans:** Zipf, G.K. (1949). *Human Behavior and the Principle of Least Effort*.
> Addison-Wesley.
""")

        # â”€â”€ 3. VektÃ¶rizasyon â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with st.spinner("CountVectorizerâ€¦"):
            text_chunks = [" ".join(tl) for tl in all_token_lists]

            effective_min_df = 1 if n_chunks < 3 else MIN_DF
            effective_max_df = 1.0 if n_chunks < 3 else MAX_DF

            vectorizer = CountVectorizer(
                min_df=effective_min_df,
                max_df=effective_max_df,
                ngram_range=(1, 1),
            )
            dtm = vectorizer.fit_transform(text_chunks)
            feature_names = vectorizer.get_feature_names_out().tolist()

        if len(feature_names) == 0:
            st.error(
                "VektÃ¶rizasyon sonrasÄ± sÃ¶zlÃ¼k boÅŸ kaldÄ±. "
                "ParÃ§a boyutunu kÃ¼Ã§Ã¼ltÃ¼n veya POS filtresini geniÅŸletin."
            )
            st.stop()

        st.success(f"âœ… SÃ¶zlÃ¼k: {len(feature_names)} terim | DTM: {dtm.shape}")

        # â”€â”€ 4. LDA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        actual_topics = min(n_topics, n_chunks, len(feature_names))
        if actual_topics < n_topics:
            st.warning(
                f"ParÃ§a/terim sayÄ±sÄ± yetersiz â†’ K={actual_topics} olarak ayarlandÄ±."
            )

        with st.spinner(f"LDA eÄŸitiliyor (K={actual_topics}, iter={max_iter})â€¦"):
            lda = LatentDirichletAllocation(
                n_components=actual_topics,
                random_state=SEED,
                learning_method="batch",
                max_iter=max_iter,
            )
            doc_topic = lda.fit_transform(dtm)

        perplexity_val = lda.perplexity(dtm)
        log_likelihood_val = lda.score(dtm)

        # Coherence
        with st.spinner("Coherence skoru hesaplanÄ±yorâ€¦"):
            coherence_val = compute_coherence(
                lda, feature_names, all_token_lists, n_words
            )

        # â”€â”€ 5. Ã‡Ä±ktÄ±larÄ± hazÄ±rla â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        topics_txt = build_topics_text(lda, feature_names, n_words)

        topic_cols = [f"Topic_{i}" for i in range(actual_topics)]
        df_dist = pd.DataFrame(doc_topic, columns=topic_cols)
        df_dist.insert(0, "ParÃ§a", chunk_labels)

        coherence_str = f"{coherence_val:.4f}" if coherence_val else "hesaplanamadÄ±"
        metrics_txt = (
            f"Perplexity     : {perplexity_val:.4f}\n"
            f"Log-likelihood : {log_likelihood_val:.4f}\n"
            f"Coherence (C_v): {coherence_str}"
        )
        params_txt = get_model_parameters(
            spacy_model, effective_min_df, effective_max_df,
            actual_topics, max_iter, chunk_size, pos_tags, custom_stopwords,
        )
        env_txt = get_environment_report(spacy_model)
        env_txt += (
            f"\nToplam belge sayÄ±sÄ± : {len(filenames)}"
            f"\nToplam parÃ§a sayÄ±sÄ± : {n_chunks}"
        )

        # Terminal
        print("\n" + "=" * 60)
        print("LDA TOPIC MODELLING â€” ORTAM BÄ°LGÄ°LERÄ°")
        print("=" * 60)
        print(env_txt)
        print("=" * 60 + "\n")

        # â”€â”€ 6. SonuÃ§larÄ± gÃ¶ster â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.divider()
        st.header("ğŸ“Š SonuÃ§lar")

        # Metrikler
        m1, m2, m3, m4, m5 = st.columns(5)
        m1.metric("Belgeler", len(filenames), help="YÃ¼klenen dosya sayÄ±sÄ±.")
        m2.metric(
            "ParÃ§alar", n_chunks,
            help="Ã–n-iÅŸleme ve parÃ§alama sonrasÄ± LDA'ya verilen toplam metin birimi.",
        )
        m3.metric(
            "Perplexity", f"{perplexity_val:.2f}",
            help="Modelin metni ne kadar iyi tahmin ettiÄŸini Ã¶lÃ§er.\n\n"
                 "â€¢ DÃ¼ÅŸÃ¼k perplexity â†’ model metne daha iyi uyum saÄŸlamÄ±ÅŸ.\n\n"
                 "â€¢ FarklÄ± K deÄŸerleriyle karÅŸÄ±laÅŸtÄ±rÄ±n.\n\n"
                 "âš ï¸ Tek baÅŸÄ±na yeterli deÄŸildir; coherence ile birlikte deÄŸerlendirin.",
        )
        m4.metric(
            "Log-Likelihood", f"{log_likelihood_val:.2f}",
            help="Modelin veriye atadÄ±ÄŸÄ± olasÄ±lÄ±ÄŸÄ±n logaritmasÄ±. "
                 "YÃ¼ksek (sÄ±fÄ±ra yakÄ±n) â†’ daha iyi uyum.",
        )
        m5.metric(
            "Coherence (C_v)",
            f"{coherence_val:.3f}" if coherence_val else "â€”",
            help="Topic'lerin insan tarafÄ±ndan ne kadar anlamlÄ± algÄ±landÄ±ÄŸÄ±nÄ± Ã¶lÃ§en standart metriktir.\n\n"
                 "**DeÄŸer aralÄ±ÄŸÄ±:** 0 ile 1 arasÄ± (yÃ¼ksek = daha iyi).\n\n"
                 "â€¢ **> 0.55** â†’ Topic'ler genellikle anlamlÄ± ve tutarlÄ±.\n\n"
                 "â€¢ **0.40 â€“ 0.55** â†’ Kabul edilebilir, ama iyileÅŸtirme mÃ¼mkÃ¼n.\n\n"
                 "â€¢ **< 0.40** â†’ Topic'ler muhtemelen anlamlÄ± deÄŸil; K'yÄ±, parÃ§a boyutunu "
                 "veya POS filtresini deÄŸiÅŸtirmeyi deneyin.\n\n"
                 "**Perplexity ile farkÄ±:** Perplexity modelin istatistiksel uyumunu, "
                 "coherence ise topic'lerin insani yorumlanabilirliÄŸini Ã¶lÃ§er. "
                 "DÃ¼ÅŸÃ¼k perplexity ama dÃ¼ÅŸÃ¼k coherence mÃ¼mkÃ¼ndÃ¼r â€” her ikisine de bakÄ±n.\n\n"
                 "**NasÄ±l iyileÅŸtirilir?**\n"
                 "1. FarklÄ± K deÄŸerleri deneyin.\n"
                 "2. POS filtresini daraltÄ±n (sadece NOUN).\n"
                 "3. Ã–zel stopword ekleyin.\n"
                 "4. ParÃ§a boyutunu ayarlayÄ±n.",
        )

        # â”€â”€ Metriklerin yorumu â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with st.expander("ğŸ“ Bu Metrikler Ne Anlama Geliyor?"):
            coh_note = ""
            if coherence_val is not None:
                if coherence_val > 0.55:
                    coh_note = ("**Coherence sonucunuz ({:.3f}):** YÃ¼ksek. "
                                "Topic'leriniz muhtemelen anlamlÄ± ve tutarlÄ±. "
                                "Kelime listelerine bakarak etiketleme aÅŸamasÄ±na geÃ§ebilirsiniz.").format(coherence_val)
                elif coherence_val > 0.40:
                    coh_note = ("**Coherence sonucunuz ({:.3f}):** Kabul edilebilir, "
                                "ama iyileÅŸtirme mÃ¼mkÃ¼n. FarklÄ± K deÄŸerleri veya POS filtresi "
                                "deneyin. Ã–zel stopword eklemeyi dÃ¼ÅŸÃ¼nÃ¼n.").format(coherence_val)
                else:
                    coh_note = ("**Coherence sonucunuz ({:.3f}):** DÃ¼ÅŸÃ¼k. "
                                "Topic'ler muhtemelen anlamlÄ± deÄŸil. ÅunlarÄ± deneyin:\n"
                                "1. K'yÄ± deÄŸiÅŸtirin (dÃ¼ÅŸÃ¼rÃ¼n veya artÄ±rÄ±n)\n"
                                "2. POS filtresini daraltÄ±n (sadece NOUN)\n"
                                "3. Ã–zel stopword ekleyin\n"
                                "4. ParÃ§a boyutunu ayarlayÄ±n").format(coherence_val)
            st.markdown(f"""
**ÃœÃ§ metriÄŸin birlikte okunmasÄ±:**

| Metrik | Ne Ã¶lÃ§er? | Ä°yi yÃ¶nÃ¼ |
|--------|-----------|----------|
| **Coherence (C_v)** | Topic'ler insana anlamlÄ± mÄ±? | YÃ¼ksek (> 0.55) |
| **Perplexity** | Model veriyi ne kadar iyi tahmin ediyor? | DÃ¼ÅŸÃ¼k |
| **Log-likelihood** | Verinin model altÄ±nda olasÄ±lÄ±ÄŸÄ± | SÄ±fÄ±ra yakÄ±n |

**Hangisine bakmalÄ±yÄ±m?** Ã–ncelikli olarak **Coherence (C_v)** skoruna bakÄ±n.
Perplexity ve log-likelihood istatistiksel uyumu Ã¶lÃ§er ama dÃ¼ÅŸÃ¼k perplexity
her zaman anlamlÄ± topic'ler demek deÄŸildir (Chang et al., 2009).

{coh_note}

> Chang, J. et al. (2009). "Reading Tea Leaves: How Humans Interpret Topic Models."
> *NeurIPS.*
""")

        # â”€â”€ Topic bar chart + word cloud â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.subheader(
            f"Konular (K={actual_topics})",
            help="Her topic, o konuyu en Ã§ok temsil eden kelimelerin aÄŸÄ±rlÄ±klÄ± listesidir.\n\n"
                 "â€¢ Kelimelerine bakarak topic'e etiket vermeye Ã§alÄ±ÅŸÄ±n.\n\n"
                 "â€¢ Benzer kelimeler iÃ§eren topic'ler varsa K'yÄ± dÃ¼ÅŸÃ¼rÃ¼n.\n\n"
                 "â€¢ Bir topic Ã§ok farklÄ± temalarÄ± karÄ±ÅŸtÄ±rÄ±yorsa K'yÄ± artÄ±rÄ±n.",
        )

        with st.expander("ğŸ·ï¸ Topic'leri NasÄ±l Etiketleriz?"):
            st.markdown("""
**Etiketleme, topic modelling'in en Ã¶nemli insani adÄ±mÄ±dÄ±r.**

Model size kelime listeleri verir, ama bu kelimelerin ne anlama geldiÄŸine
siz karar verirsiniz. Ã–rnek:

| Topic Kelimeleri | OlasÄ± Etiket |
|-----------------|--------------|
| cuore, amore, donna, dolore, occhi | AÅŸk ve duygu |
| guerra, soldato, morte, sangue, nemico | SavaÅŸ ve ÅŸiddet |
| casa, famiglia, madre, figlio, porta | Aile ve ev |
| mare, cielo, sole, terra, vento | DoÄŸa ve manzara |

**Etiketleme ipuÃ§larÄ±:**
1. Ä°lk 5 kelimeye odaklanÄ±n â€” bunlar topic'in "Ã§ekirdeÄŸidir".
2. Kelime bulutu'nda bÃ¼yÃ¼k gÃ¶rÃ¼nen kelimelere bakÄ±n.
3. Birden fazla tema gÃ¶rÃ¼yorsanÄ±z â†’ K'yÄ± artÄ±rmayÄ± deneyin.
4. Etiket bulamÄ±yorsanÄ±z â†’ topic muhtemelen anlamsÄ±z ("junk topic").
   Bu durumda Ã¶zel stopword ekleyin veya K'yÄ± deÄŸiÅŸtirin.

**Akademik raporlamada:** Topic etiketleri araÅŸtÄ±rmacÄ±nÄ±n yorumudur.
"Topic 0: AÅŸk ve duygu (cuore, amore, donna, dolore, occhi)"
ÅŸeklinde hem etiket hem kelimeler birlikte verilir.
""")

        for idx, topic_dist in enumerate(lda.components_):
            top_indices = topic_dist.argsort()[: -n_words - 1 : -1]
            top_words = [feature_names[i] for i in top_indices]
            weights = [topic_dist[i] for i in top_indices]

            st.markdown(f"**Topic {idx}**")
            col_bar, col_wc = st.columns([3, 2])

            with col_bar:
                bar_df = pd.DataFrame({"Kelime": top_words, "AÄŸÄ±rlÄ±k": weights})
                st.bar_chart(bar_df, x="Kelime", y="AÄŸÄ±rlÄ±k", height=220)

            with col_wc:
                wc_indices = topic_dist.argsort()[:-51:-1]
                ww = {feature_names[i]: float(topic_dist[i]) for i in wc_indices}
                fig_wc = generate_wordcloud(ww)
                st.pyplot(fig_wc, use_container_width=True)
                plt.close(fig_wc)

        # â”€â”€ pyLDAvis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.subheader("ğŸ—ºï¸ pyLDAvis Ä°nteraktif Harita")
        with st.expander("â„¹ï¸ pyLDAvis nasÄ±l yorumlanÄ±r?", expanded=True):
            st.markdown("""
**pyLDAvis**, Sievert ve Shirley (2014) tarafÄ±ndan geliÅŸtirilen interaktif
topic model gÃ¶rselleÅŸtirme aracÄ±dÄ±r. Ä°ki panelden oluÅŸur:

---

**Sol panel â€” Topic daireleri (Intertopic Distance Map):**
- Dairenin **bÃ¼yÃ¼klÃ¼ÄŸÃ¼** = topic'in korpustaki genel aÄŸÄ±rlÄ±ÄŸÄ± (sÄ±klÄ±ÄŸÄ±).
  BÃ¼yÃ¼k daire = o konu metinlerde daha baskÄ±n.
- Daireler arasÄ± **mesafe** = topic'ler arasÄ±ndaki benzerlik.
  - **Birbirinden uzak daireler** â†’ topic'ler birbirinden ayrÄ±ÅŸÄ±yor (iyi!).
  - **Ãœst Ã¼ste binen daireler** â†’ topic'ler benzer kelimeler paylaÅŸÄ±yor.
    Bu durumda **K'yÄ± dÃ¼ÅŸÃ¼rmeyi** deneyin.
- Koordinatlar PCA (Principal Component Analysis) ile hesaplanÄ±r;
  yÃ¼ksek boyutlu topic-kelime daÄŸÄ±lÄ±mlarÄ±nÄ± 2 boyuta indirger.

**SaÄŸ panel â€” Kelime Ã§ubuklarÄ±:**
- Bir daireye **tÄ±klayÄ±n** â†’ o topic'in en aÄŸÄ±rlÄ±klÄ± kelimeleri gÃ¶rÃ¼nÃ¼r.
- **KÄ±rmÄ±zÄ± Ã§ubuk** = kelimenin **o topic'teki** sÄ±klÄ±ÄŸÄ± (topic-spesifik).
- **Mavi Ã§ubuk** = kelimenin **genel korpustaki** sÄ±klÄ±ÄŸÄ± (overall).
- **KÄ±rmÄ±zÄ± â‰« mavi** â†’ o kelime bu topic'e **Ã¶zgÃ¼dÃ¼r** (ayÄ±rt edici).
- **KÄ±rmÄ±zÄ± â‰ˆ mavi** â†’ kelime her yerde geÃ§iyor, topic'e Ã¶zgÃ¼ deÄŸil.
  Bu tÃ¼r kelimeleri Ã¶zel stopword'e eklemeyi dÃ¼ÅŸÃ¼nÃ¼n.

---

**Î» (lambda) slider'Ä± â€” en Ã¶nemli kontrol:**

| Î» deÄŸeri | Ne gÃ¶sterir? | Ne zaman? |
|----------|-------------|-----------|
| **Î» = 1** | Mutlak sÄ±klÄ±ÄŸa gÃ¶re sÄ±ralar | Topic'in en sÄ±k kelimelerini gÃ¶rmek iÃ§in |
| **Î» = 0** | AyÄ±rt ediciliÄŸe gÃ¶re sÄ±ralar | Topic'e Ã¶zgÃ¼ kelimeleri bulmak iÃ§in |
| **Î» â‰ˆ 0.6** | SÄ±klÄ±k ve ayÄ±rt edicilik dengesi | **Ã–nerilen baÅŸlangÄ±Ã§** (Sievert & Shirley, 2014) |

**Pratik yÃ¶ntem:** Î»'yÄ± 1'den 0'a doÄŸru kaydÄ±rÄ±n. Kelimelerin sÄ±ralamasÄ± deÄŸiÅŸecektir.
Î»=1'de Ã¼st sÄ±radaki genel kelimeler, Î»=0'a yaklaÅŸtÄ±kÃ§a yerini o topic'e Ã¶zgÃ¼
kelimelere bÄ±rakÄ±r. Bu deÄŸiÅŸimi gÃ¶zlemlemek topic'in karakterini anlamanÄ±zÄ± saÄŸlar.

> Sievert, C. & Shirley, K. (2014). "LDAvis: A method for visualizing and
> interpreting topics." *ACL Workshop on Interactive Language Learning,
> Visualization, and Interfaces.*
""")

        with st.spinner("pyLDAvis hazÄ±rlanÄ±yorâ€¦"):
            vis_html = render_pyldavis(lda, dtm, vectorizer)
        if vis_html:
            st.components.v1.html(vis_html, height=800, scrolling=True)
        else:
            st.info("pyLDAvis yÃ¼klenemedi. `pip install pyLDAvis`")

        # â”€â”€ IsÄ± haritasÄ± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.subheader(
            "IsÄ± HaritasÄ±",
            help="Her hÃ¼cre, bir metin parÃ§asÄ±nÄ±n belirli bir topic ile iliÅŸki olasÄ±lÄ±ÄŸÄ±nÄ± gÃ¶sterir.\n\n"
                 "â€¢ **Koyu mavi** = yÃ¼ksek olasÄ±lÄ±k (gÃ¼Ã§lÃ¼ iliÅŸki).\n\n"
                 "â€¢ **AÃ§Ä±k mavi / beyaz** = dÃ¼ÅŸÃ¼k olasÄ±lÄ±k (zayÄ±f iliÅŸki).\n\n"
                 "**NasÄ±l yorumlanÄ±r?**\n\n"
                 "â€¢ Bir satÄ±rda **tek koyu hÃ¼cre** â†’ o parÃ§a net bir konuya ait (monotematik).\n\n"
                 "â€¢ Bir satÄ±rda **birden fazla koyu hÃ¼cre** â†’ o parÃ§a birden fazla konu iÃ§eriyor (Ã§oklu tema). "
                 "Bu, edebiyat metinlerinde normaldir â€” bir paragraf hem aÅŸk hem doÄŸa temasÄ± taÅŸÄ±yabilir.\n\n"
                 "â€¢ Bir **sÃ¼tun tamamen koyu** â†’ o topic metinlerin tÃ¼mÃ¼ne yayÄ±lmÄ±ÅŸ (dominant topic). "
                 "Ã‡ok baskÄ±n bir topic varsa, o topic'in kelimelerini stopword'e eklemeyi dÃ¼ÅŸÃ¼nÃ¼n.\n\n"
                 "**Akademik kullanÄ±m:** IsÄ± haritasÄ±, metnin bÃ¶lÃ¼mleri boyunca tematik dÃ¶nÃ¼ÅŸÃ¼mÃ¼ "
                 "gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lÄ±r. Roman analizlerinde bÃ¶lÃ¼mler arasÄ± tema geÃ§iÅŸleri "
                 "bu haritadan okunabilir.",
        )
        heat_data = df_dist.melt(
            id_vars="ParÃ§a", var_name="Topic", value_name="OlasÄ±lÄ±k"
        )
        heatmap = (
            alt.Chart(heat_data)
            .mark_rect()
            .encode(
                x=alt.X("Topic:N", title="Topic"),
                y=alt.Y("ParÃ§a:N", title="ParÃ§a", sort=None),
                color=alt.Color("OlasÄ±lÄ±k:Q", scale=alt.Scale(scheme="blues")),
                tooltip=["ParÃ§a", "Topic", alt.Tooltip("OlasÄ±lÄ±k:Q", format=".4f")],
            )
            .properties(height=max(250, n_chunks * 22))
        )
        st.altair_chart(heatmap, use_container_width=True)

        # â”€â”€ DaÄŸÄ±lÄ±m tablosu â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.subheader(
            "ParÃ§a â€” Topic DaÄŸÄ±lÄ±mÄ±",
            help="Her satÄ±r bir metin parÃ§asÄ±nÄ±n topic olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶sterir.\n\n"
                 "â€¢ Her satÄ±rÄ±n toplamÄ± **1.0** (= %100)'dir.\n\n"
                 "â€¢ **0.82** = o parÃ§anÄ±n %82'si bu konuyla iliÅŸkili.\n\n"
                 "â€¢ **0.05** = neredeyse hiÃ§ iliÅŸki yok.\n\n"
                 "Bu tablo, Ä±sÄ± haritasÄ±nÄ±n sayÄ±sal karÅŸÄ±lÄ±ÄŸÄ±dÄ±r. "
                 "CSV olarak indirip kendi analizlerinizde (Ã¶r. R veya Excel) kullanabilirsiniz.",
        )
        st.dataframe(
            df_dist.style.format({c: "{:.4f}" for c in topic_cols}),
            use_container_width=True,
        )

        # â”€â”€ BaskÄ±n topic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.subheader(
            "BaskÄ±n Topic (ParÃ§a BaÅŸÄ±na)",
            help="Her parÃ§a iÃ§in en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip topic ve gÃ¼ven skoru.\n\n"
                 "**GÃ¼ven skoru yorumlama:**\n\n"
                 "| GÃ¼ven | Yorum |\n"
                 "|-------|-------|\n"
                 "| **> 0.70** | ParÃ§a net bir konuya odaklÄ± (monotematik) |\n"
                 "| **0.40 â€“ 0.70** | Ã‡oklu tema var ama bir tanesi baskÄ±n |\n"
                 "| **< 0.40** | ParÃ§a tematik olarak daÄŸÄ±nÄ±k â€” birden fazla konu eÅŸit aÄŸÄ±rlÄ±kta |\n\n"
                 "**Edebiyat analizi iÃ§in:** DÃ¼ÅŸÃ¼k gÃ¼ven skorlarÄ± her zaman kÃ¶tÃ¼ deÄŸildir. "
                 "Bir roman bÃ¶lÃ¼mÃ¼ hem savaÅŸ hem aÅŸk temasÄ±nÄ± eÅŸit aÄŸÄ±rlÄ±kta iÅŸleyebilir. "
                 "Ã–nemli olan, bu daÄŸÄ±lÄ±mÄ±n metnin iÃ§eriÄŸiyle tutarlÄ± olup olmadÄ±ÄŸÄ±dÄ±r â€” "
                 "yakÄ±n okuma (*close reading*) ile doÄŸrulayÄ±n.",
        )
        dominant = df_dist.copy()
        dominant["BaskÄ±n Topic"] = doc_topic.argmax(axis=1)
        dominant["GÃ¼ven"] = doc_topic.max(axis=1)
        st.dataframe(
            dominant[["ParÃ§a", "BaskÄ±n Topic", "GÃ¼ven"]].style.format({"GÃ¼ven": "{:.4f}"}),
            use_container_width=True,
        )

        # â”€â”€ Akademik raporlama rehberi â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with st.expander("ğŸ“ SonuÃ§larÄ± Akademik Makalede NasÄ±l RaporlarÄ±m?"):
            st.markdown(f"""
**Bir DH makalesinde topic modelling sonuÃ§larÄ±nÄ± raporlarken ÅŸu bilgileri mutlaka verin:**

**1. Veri ve Ã¶n-iÅŸleme:**
- Korpus bÃ¼yÃ¼klÃ¼ÄŸÃ¼: {len(filenames)} belge, {n_chunks} parÃ§a
- Dil modeli: `{spacy_model}`
- POS filtresi: {', '.join(pos_tags)}
- Ã–zel stopword: {', '.join(sorted(custom_stopwords)) if custom_stopwords else 'yok'}
- ParÃ§a boyutu: {chunk_size} kelime
- SÃ¶zlÃ¼k boyutu: {len(feature_names)} terim

**2. Model parametreleri:**
- Algoritma: LDA (scikit-learn LatentDirichletAllocation)
- K (topic sayÄ±sÄ±): {actual_topics}
- Ã–ÄŸrenme yÃ¶ntemi: batch
- Iterasyon: {max_iter}
- Seed: {SEED}

**3. DeÄŸerlendirme metrikleri:**
- Coherence (C_v): {f"{coherence_val:.4f}" if coherence_val else "hesaplanamadÄ±"}
- Perplexity: {perplexity_val:.4f}
- Log-likelihood: {log_likelihood_val:.4f}

**4. FarklÄ± K denemeleri:**
Akademik standartta en az 3 farklÄ± K denenmeli ve sonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±rÄ±lmalÄ±dÄ±r.
Neden bu K'yÄ± seÃ§tiÄŸinizi gerekÃ§elendirin (coherence skoru + insani deÄŸerlendirme).

**5. Topic etiketleri:**
Her topic'e araÅŸtÄ±rmacÄ± olarak verdiÄŸiniz etiketler ve gerekÃ§eler.

**6. Tekrar edilebilirlik:**
"TÃ¼m analizler seed=42 ile yapÄ±lmÄ±ÅŸtÄ±r. Paket sÃ¼rÃ¼mleri ve ortam bilgisi
indirilebilir Ã§Ä±ktÄ±larda mevcuttur."

**Ã–rnek paragraf:**
> "Korpus, LDA topic modelling ile analiz edilmiÅŸtir (K={actual_topics}, batch learning,
> seed=42). Ã–n-iÅŸleme aÅŸamasÄ±nda spaCy `{spacy_model}` modeli ile lemmatizasyon
> yapÄ±lmÄ±ÅŸ, yalnÄ±zca {', '.join(pos_tags)} sÃ¶zcÃ¼k tÃ¼rleri dahil edilmiÅŸtir.
> Coherence (C_v) skoru {f'{coherence_val:.3f}' if coherence_val else 'â€”'} olarak
> hesaplanmÄ±ÅŸtÄ±r (RÃ¶der et al., 2015)."
""")

        # â”€â”€ Ortam & parametre bilgisi â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with st.expander("ğŸ–¥ï¸ Ortam Bilgisi"):
            st.code(env_txt, language="text")
        with st.expander("âš™ï¸ Model Parametreleri"):
            st.code(params_txt, language="text")

        # â”€â”€ Ä°ndirme â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.divider()
        st.subheader("ğŸ“¥ Ã‡Ä±ktÄ±larÄ± Ä°ndir")

        csv_buf = io.StringIO()
        df_dist.to_csv(csv_buf, index=False)

        # Derlem istatistikleri raporu
        corpus_stats_lines = [
            "â•â•â• DERLEM Ä°STATÄ°STÄ°KLERÄ° â•â•â•",
            f"Ham sÃ¶zcÃ¼k sayÄ±sÄ±       : {corpus_stats['raw_word_count']:,}",
            f"Ham karakter sayÄ±sÄ±     : {corpus_stats['raw_char_count']:,}",
            f"Temiz token sayÄ±sÄ±      : {corpus_stats['clean_token_count']:,}",
            f"Benzersiz kelime (type) : {corpus_stats['clean_type_count']:,}",
            f"Type-Token Ratio (TTR)  : {corpus_stats['ttr']:.4f}",
            f"Hapax legomena          : {corpus_stats['hapax_count']:,}",
            f"Ort. parÃ§a uzunluÄŸu     : {corpus_stats['avg_chunk_len']:.0f} token",
            f"Filtreleme oranÄ±        : {(1 - corpus_stats['clean_token_count'] / corpus_stats['raw_word_count']) * 100:.1f}%",
            "",
            "â•â•â• EN SIK 50 KELÄ°ME â•â•â•",
        ]
        for rank, (word, count) in enumerate(corpus_stats["top_50"], 1):
            corpus_stats_lines.append(f"{rank:>3}. {word:<25} {count:>6}")
        corpus_stats_txt = "\n".join(corpus_stats_lines)

        # Frekans CSV
        freq_csv_buf = io.StringIO()
        freq_df_all = pd.DataFrame(
            sorted(corpus_stats["freq"].items(), key=lambda x: -x[1]),
            columns=["Kelime", "Frekans"],
        )
        freq_df_all.to_csv(freq_csv_buf, index=False)

        output_files = {
            "topics.txt": topics_txt,
            "doc_topic_distribution.csv": csv_buf.getvalue(),
            "corpus_statistics.txt": corpus_stats_txt,
            "word_frequencies.csv": freq_csv_buf.getvalue(),
            "metrics.txt": metrics_txt,
            "model_parameters.txt": params_txt,
            "environment_report.txt": env_txt,
        }

        zip_bytes = create_zip(output_files)
        st.download_button(
            label="â¬‡ TÃ¼m Ã§Ä±ktÄ±larÄ± indir (ZIP)",
            data=zip_bytes,
            file_name="lda_outputs.zip",
            mime="application/zip",
            use_container_width=True,
        )

        with st.expander("DosyalarÄ± ayrÄ± ayrÄ± indir"):
            for fname, content in output_files.items():
                st.download_button(
                    label=fname,
                    data=content,
                    file_name=fname,
                    mime="text/plain" if fname.endswith(".txt") else "text/csv",
                )

else:
    # HoÅŸgeldin ekranÄ±
    st.markdown("""
### NasÄ±l KullanÄ±lÄ±r

1. Sol panelden **dil**, **POS filtresi** ve **parametreleri** ayarlayÄ±n.
2. YukarÄ±daki kutuya `.txt`, `.odt` veya `.pdf` dosyalarÄ±nÄ±zÄ± sÃ¼rÃ¼kleyin.
3. **â–¶ Analizi BaÅŸlat** dÃ¼ÄŸmesine tÄ±klayÄ±n.
4. SonuÃ§larÄ± ekranda inceleyin, ZIP olarak indirin.

| AdÄ±m | Ne olur? |
|------|----------|
| **Ã–n-iÅŸleme** | Metin â†’ token â†’ POS filtre â†’ lemma â†’ temiz kelime listesi |
| **ParÃ§alama** | Tek uzun metin otomatik parÃ§alara bÃ¶lÃ¼nÃ¼r |
| **VektÃ¶rizasyon** | Kelimeler â†’ sayÄ±sal matris (CountVectorizer) |
| **LDA** | Gizli konular (topics) hesaplanÄ±r |
| **GÃ¶rselleÅŸtirme** | Bar chart, kelime bulutu, Ä±sÄ± haritasÄ±, pyLDAvis |
""")

    st.info(
        "ğŸ’¡ Tek kitap da yÃ¼klenebilir. Metin otomatik olarak parÃ§alara "
        "bÃ¶lÃ¼nÃ¼r ve her parÃ§a bir belge gibi analiz edilir."
    )

    with st.expander("ğŸ“ Parametre Rehberi: Nereden BaÅŸlamalÄ±?"):
        st.markdown("""
**1. Konu sayÄ±sÄ± (K) â€” en kritik karar:**

| Durum | Ã–neri | GerekÃ§e |
|---|---|---|
| KÄ±sa metin (< 5.000 kelime) | K = 2â€“3 | Az veri, fazla topic anlamsÄ±z olur |
| Orta metin (5.000â€“50.000 kelime) | K = 3â€“7 | Standart aralÄ±k |
| Uzun metin / birden fazla eser | K = 5â€“12 | Daha fazla tematik Ã§eÅŸitlilik beklenir |
| BÃ¼yÃ¼k korpus (10+ eser) | K = 8â€“15 | GeniÅŸ tematik yelpaze |

**Akademik yÃ¶ntem:** En az **3 farklÄ± K** deneyin (Ã¶r. 3, 5, 7) ve Coherence
skorlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rÄ±n. En yÃ¼ksek C_v â†’ en anlamlÄ± topic yapÄ±sÄ±. Bunu
insani deÄŸerlendirme ile doÄŸrulayÄ±n: "Topic kelimelerine bakarak anlamlÄ±
bir etiket verebiliyor muyum?"

**2. POS Filtresi â€” akademik tercihler:**

| KonfigÃ¼rasyon | Ne zaman? | Akademik referans |
|---|---|---|
| Sadece **NOUN** | Ä°lk deney, en temiz sonuÃ§lar | Schofield & Mimno (2016) |
| **NOUN + ADJ** | Duygusal/tanÄ±mlayÄ±cÄ± temalar gerektiÄŸinde | Sentiment-yakÄ±n analiz |
| **NOUN + ADJ + VERB** | AnlatÄ± ve eylem analizi | Narratoloji Ã§alÄ±ÅŸmalarÄ± |
| **NOUN + PROPN** | Karakter/yer adÄ± analizi | NER tarzÄ± Ã§alÄ±ÅŸmalar |

**Ã–neri:** Sadece NOUN ile baÅŸlayÄ±n. Her POS eklediÄŸinizde coherence'Ä±n nasÄ±l
deÄŸiÅŸtiÄŸini gÃ¶zlemleyin.

**3. ParÃ§a boyutu â€” veri yapÄ±sÄ±na gÃ¶re:**

| Boyut | Ne zaman? | Dikkat |
|---|---|---|
| 50â€“100 kelime | Åiir, kÄ±sa metin, paragraf analizi | GÃ¼rÃ¼ltÃ¼lÃ¼ olabilir |
| 150â€“250 kelime | Roman, uzun deneme (**Ã¶nerilen**) | Denge noktasÄ± |
| 300â€“500 kelime | Ã‡ok uzun metin, genel tema taramasÄ± | Ä°nce ayrÄ±mlar kaybolur |

**4. Ä°terasyon:**
- **20** (varsayÄ±lan) Ã§oÄŸu durum iÃ§in yeterlidir.
- SonuÃ§lar 20 ile 30 arasÄ±nda deÄŸiÅŸmiyorsa model yakÄ±nsamÄ±ÅŸtÄ±r.
- ArtÄ±rmak Ã§alÄ±ÅŸma sÃ¼resini uzatÄ±r ama sonucu iyileÅŸtirmeyebilir.

**5. DeÄŸerlendirme kontrol listesi:**
- [ ] En az 3 farklÄ± K deneyin (3, 5, 7)
- [ ] Her denemede Coherence (C_v) skorunu kaydedin
- [ ] En yÃ¼ksek coherence veren K'yÄ± seÃ§in
- [ ] Topic kelimelerini okuyun: anlamlÄ± temalar oluÅŸuyor mu?
- [ ] AnlamsÄ±z/tekrar eden kelimeler varsa Ã¶zel stopword'e ekleyin
- [ ] pyLDAvis'te dairelerin birbirinden ayrÄ±ÅŸÄ±p ayrÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol edin
- [ ] POS filtresini deÄŸiÅŸtirip etkisini gÃ¶zlemleyin
- [ ] SonuÃ§larÄ± close reading ile doÄŸrulayÄ±n
""")

    with st.expander("ğŸ§ª Deney TasarÄ±mÄ±: AdÄ±m AdÄ±m Analiz YÃ¶ntemi"):
        st.markdown("""
**Bir edebiyat metni Ã¼zerinde sistematik LDA analizi nasÄ±l yapÄ±lÄ±r?**

```
AÅAMA 1: KeÅŸif (Exploratory)
â”œâ”€ VarsayÄ±lan parametrelerle ilk Ã§alÄ±ÅŸtÄ±rma
â”œâ”€ Topic kelimelerine gÃ¶z atma
â”œâ”€ AnlamsÄ±z kelimeleri not alma
â””â”€ Genel izlenim edinme

AÅAMA 2: Parametre Optimizasyonu
â”œâ”€ K = 3 â†’ Coherence skorunu kaydet
â”œâ”€ K = 5 â†’ Coherence skorunu kaydet
â”œâ”€ K = 7 â†’ Coherence skorunu kaydet
â”œâ”€ K = 10 â†’ Coherence skorunu kaydet
â”œâ”€ En iyi K'yÄ± seÃ§ (yÃ¼ksek coherence + anlamlÄ± topic'ler)
â”œâ”€ Ã–zel stopword ekle â†’ tekrar Ã§alÄ±ÅŸtÄ±r
â””â”€ POS filtresini dene (NOUN â†’ NOUN+ADJ â†’ NOUN+ADJ+VERB)

AÅAMA 3: Yorumlama
â”œâ”€ Her topic'e etiket ver
â”œâ”€ pyLDAvis ile topic iliÅŸkilerini incele
â”œâ”€ IsÄ± haritasÄ±nda tematik dÃ¶nÃ¼ÅŸÃ¼mleri gÃ¶zlemle
â”œâ”€ BaskÄ±n topic tablosu ile metin yapÄ±sÄ±nÄ± analiz et
â””â”€ Close reading ile doÄŸrula

AÅAMA 4: Raporlama
â”œâ”€ TÃ¼m parametreleri belge
â”œâ”€ FarklÄ± K denemelerini raporla
â”œâ”€ SeÃ§im gerekÃ§elerini aÃ§Ä±kla
â”œâ”€ GÃ¶rselleÅŸtirmeleri ekle
â””â”€ Tekrar edilebilirlik bilgisini ver (seed, paket sÃ¼rÃ¼mleri)
```

**Ã–nemli:** Her aÅŸamada sonuÃ§larÄ± **not alÄ±n**. "K=5'te coherence 0.52,
K=7'de 0.48 â€” K=5 tercih edildi Ã§Ã¼nkÃ¼ topic'ler daha anlamlÄ± etiketlenebiliyordu."
Bu tÃ¼r notlar akademik makalenizin metodoloji bÃ¶lÃ¼mÃ¼nÃ¼ oluÅŸturur.
""")

    with st.expander("ğŸ“š Akademik Kaynaklar"):
        st.markdown("""
**Topic Modelling temel kaynaklarÄ±:**

| Kaynak | KatkÄ±sÄ± |
|--------|---------|
| Blei, Ng & Jordan (2003). "Latent Dirichlet Allocation." *JMLR* | LDA'nÄ±n orijinal makalesi |
| Blei (2012). "Probabilistic Topic Models." *CACM* | EriÅŸilebilir genel bakÄ±ÅŸ |
| RÃ¶der, Both & Hinneburg (2015). "Exploring the Space of Topic Coherence Measures." *WSDM* | C_v coherence metriÄŸi |
| Chang et al. (2009). "Reading Tea Leaves." *NeurIPS* | Perplexity â‰  insani yorum |
| Sievert & Shirley (2014). "LDAvis." *ACL Workshop* | pyLDAvis gÃ¶rselleÅŸtirmesi |
| Schofield & Mimno (2016). "Comparing Apples to Apple." *ACL Workshop* | POS filtreleme etkisi |
| Schofield, Magnusson & Mimno (2017). "Pulling Out the Stops." *EACL* | Stopword etkisi |

**Dijital BeÅŸeri Bilimler kaynaklarÄ±:**

| Kaynak | KatkÄ±sÄ± |
|--------|---------|
| Moretti (2013). *Distant Reading*. Verso | Uzak okuma kavramÄ± |
| Jockers (2013). *Macroanalysis*. UIUC Press | Edebiyatta hesaplamalÄ± yÃ¶ntemler |
| Underwood (2019). *Distant Horizons*. Chicago UP | Topic modelling + edebiyat tarihi |
| Graham, Weingart & Milligan (2015). *Exploring Big Historical Data*. Imperial College Press | DH'da topic modelling pratikleri |

**FaydalÄ± Ã§evrimiÃ§i kaynaklar:**
- [Programming Historian â€” Topic Modeling](https://programminghistorian.org/) â€” adÄ±m adÄ±m rehber
- [DARIAH-DE Topic Modelling](https://dariah-de.github.io/Topics/) â€” DH odaklÄ± araÃ§
""")

    with st.expander("ğŸ”‘ Temel Kavramlar SÃ¶zlÃ¼ÄŸÃ¼"):
        st.markdown("""
| Kavram | AÃ§Ä±klama |
|--------|----------|
| **Topic** | LDA'nÄ±n bulduÄŸu gizli konu â€” kelimelerin olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± |
| **Lemma** | Kelimenin sÃ¶zlÃ¼k formu: "andavo" â†’ "andare" |
| **Tokenizasyon** | Metni kelimelere ayÄ±rma iÅŸlemi |
| **POS (Part-of-Speech)** | SÃ¶zcÃ¼k tÃ¼rÃ¼: isim, fiil, sÄ±fat vb. |
| **Stopword** | Ã‡ok sÄ±k ama anlamsÄ±z kelimeler: "il", "di", "che" |
| **Coherence (C_v)** | Topic'lerin insani anlamlÄ±lÄ±ÄŸÄ±nÄ± Ã¶lÃ§en metrik (0â€“1) |
| **Perplexity** | Modelin istatistiksel uyumunu Ã¶lÃ§en metrik (dÃ¼ÅŸÃ¼k = iyi) |
| **Bag-of-Words (BoW)** | Kelime sÄ±rasÄ±nÄ± gÃ¶z ardÄ± eden temsil yÃ¶ntemi |
| **DTM (Document-Term Matrix)** | SatÄ±r = belge, sÃ¼tun = kelime, deÄŸer = sayÄ± |
| **min_df / max_df** | Ã‡ok nadir veya Ã§ok yaygÄ±n kelimeleri filtreleme eÅŸikleri |
| **Batch learning** | TÃ¼m veriyi her iterasyonda iÅŸleyen Ã¶ÄŸrenme yÃ¶ntemi |
| **Seed (random_state)** | RastgeleliÄŸi sabitleyen sayÄ± â€” tekrar edilebilirlik iÃ§in |
| **Distant reading** | Metni yakÄ±ndan okumadan hesaplamalÄ± yÃ¶ntemlerle analiz etme |
| **Close reading** | Metni dikkatli, yakÄ±ndan okuma â€” DH'da distant reading'i doÄŸrular |
| **Pseudo-belge** | Tek bir metnin parÃ§alara bÃ¶lÃ¼nmesiyle oluÅŸan yapay belge birimi |
| **Î» (lambda)** | pyLDAvis'te sÄ±klÄ±k ve ayÄ±rt edicilik arasÄ±ndaki denge kontrolÃ¼ |
""")
